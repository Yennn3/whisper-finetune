{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ac820-a785-4b7c-ba62-b701a1f7ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Remember to put vocab.json, merges.txt, ect files in the checkpoint model if they are in different folders\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba1ab35-db48-42fa-97ed-5a7505c765d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d056f99-2b39-4d96-b7da-109f388aebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the path to your fine-tuned model directory\n",
    "model_dir = \"/path/to/your/model/checkpoint-5000\"\n",
    "\n",
    "#Load the processor\n",
    "processor = WhisperProcessor.from_pretrained(model_dir)\n",
    "\n",
    "#Load the model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220f24a-aaa7-4b9d-b62c-5bc46c9ea8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load your audio file\n",
    "audio_input, original_sample_rate = sf.read(\"/path/to/audio/test.wav\")\n",
    "\n",
    "#Resample the audio to 16000 Hz\n",
    "audio_input_16000 = librosa.resample(audio_input, orig_sr=original_sample_rate, target_sr=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbd234-7f0a-4e6d-9b5a-dd9762442acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure the audio array is in the correct shape (1D array)\n",
    "if len(audio_input_16000.shape) > 1:\n",
    "    audio_input_16000 = audio_input_16000.mean(axis=1)\n",
    "\n",
    "#Preprocess the audio input using the feature extractor directly\n",
    "input_features = processor.feature_extractor(audio_input_16000, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "\n",
    "#Prepare decoder input ids (start of sequence token)\n",
    "decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
    "\n",
    "#Perform inference\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        inputs=input_features,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        max_length=225,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424dfb1-16ce-4159-ae42-1f349cc1b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decode the generated ids to get the transcription\n",
    "transcription = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Transcription:\", transcription[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
